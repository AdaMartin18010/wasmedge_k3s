# API AI/ML é›†æˆè§„èŒƒ

**ç‰ˆæœ¬**ï¼šv1.0 **æœ€åæ›´æ–°**ï¼š2025-11-07 **ç»´æŠ¤è€…**ï¼šé¡¹ç›®å›¢é˜Ÿ

## ğŸ“‘ ç›®å½•

- [ğŸ“‘ ç›®å½•](#-ç›®å½•)
- [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [1.1 AI/ML API æ¶æ„](#11-aiml-api-æ¶æ„)
- [2. æ¨¡å‹æœåŠ¡ API](#2-æ¨¡å‹æœåŠ¡-api)
  - [2.1 TensorFlow Serving](#21-tensorflow-serving)
  - [2.2 PyTorch Serve](#22-pytorch-serve)
- [3. WASM ML è¿è¡Œæ—¶](#3-wasm-ml-è¿è¡Œæ—¶)
  - [3.1 WASI-NN](#31-wasi-nn)
  - [3.2 WasmEdge ML](#32-wasmedge-ml)
- [4. æ¨¡å‹æ¨ç† API](#4-æ¨¡å‹æ¨ç†-api)
  - [4.1 RESTful æ¨ç† API](#41-restful-æ¨ç†-api)
  - [4.2 gRPC æ¨ç† API](#42-grpc-æ¨ç†-api)
- [5. æ¨¡å‹ç®¡ç†](#5-æ¨¡å‹ç®¡ç†)
  - [5.1 æ¨¡å‹ç‰ˆæœ¬ç®¡ç†](#51-æ¨¡å‹ç‰ˆæœ¬ç®¡ç†)
  - [5.2 A/B æµ‹è¯•](#52-ab-æµ‹è¯•)
- [6. æ€§èƒ½ä¼˜åŒ–](#6-æ€§èƒ½ä¼˜åŒ–)
  - [6.1 æ‰¹å¤„ç†ä¼˜åŒ–](#61-æ‰¹å¤„ç†ä¼˜åŒ–)
  - [6.2 æ¨¡å‹é‡åŒ–](#62-æ¨¡å‹é‡åŒ–)
- [7. ç›¸å…³æ–‡æ¡£](#7-ç›¸å…³æ–‡æ¡£)

---

## 1. æ¦‚è¿°

API AI/ML é›†æˆè§„èŒƒå®šä¹‰äº† API åœ¨ AI/ML ç¯å¢ƒä¸‹çš„è®¾è®¡å’Œå®ç°ï¼Œä»æ¨¡å‹æœåŠ¡åˆ°æ¨ç† APIï¼Œ
ä»æ¨¡å‹ç®¡ç†åˆ°æ€§èƒ½ä¼˜åŒ–ã€‚

### 1.1 AI/ML API æ¶æ„

```text
æ¨¡å‹è®­ç»ƒï¼ˆModel Trainingï¼‰
  â†“
æ¨¡å‹æ³¨å†Œï¼ˆModel Registryï¼‰
  â†“
æ¨¡å‹æœåŠ¡ï¼ˆModel Servingï¼‰
  â†“
æ¨ç† APIï¼ˆInference APIï¼‰
```

---

## 2. æ¨¡å‹æœåŠ¡ API

### 2.1 TensorFlow Serving

**TensorFlow Serving é…ç½®**ï¼š

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tensorflow-serving
spec:
  replicas: 3
  template:
    spec:
      containers:
        - name: tensorflow-serving
          image: tensorflow/serving:latest
          ports:
            - containerPort: 8500
            - containerPort: 8501
          env:
            - name: MODEL_NAME
              value: payment-model
            - name: MODEL_BASE_PATH
              value: /models
```

### 2.2 PyTorch Serve

**PyTorch Serve é…ç½®**ï¼š

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pytorch-serve
spec:
  replicas: 3
  template:
    spec:
      containers:
        - name: pytorch-serve
          image: pytorch/torchserve:latest
          ports:
            - containerPort: 8080
            - containerPort: 8081
          env:
            - name: MODEL_STORE
              value: /models
```

---

## 3. WASM ML è¿è¡Œæ—¶

### 3.1 WASI-NN

**WASI-NN æ¥å£å®šä¹‰**ï¼š

```wit
package wasi:nn@0.1.0;

interface nn {
    type graph = resource;
    type graph-execution-context = resource;

    load: func(
        builder: graph-builder,
        encoding: graph-encoding,
        target: execution-target
    ) -> result<graph, error>;

    init-execution-context: func(graph: graph) -> result<graph-execution-context, error>;

    set-input: func(
        ctx: graph-execution-context,
        index: u32,
        tensor: tensor
    ) -> result<(), error>;

    compute: func(ctx: graph-execution-context) -> result<(), error>;

    get-output: func(
        ctx: graph-execution-context,
        index: u32,
        out-buffer: list<u8>
    ) -> result<u32, error>;
}
```

### 3.2 WasmEdge ML

**WasmEdge ML é…ç½®**ï¼š

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wasmedge-ml
spec:
  template:
    spec:
      runtimeClassName: wasm-edge-ml
      containers:
        - name: ml-inference
          image: ml-inference.wasm
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
```

---

## 4. æ¨¡å‹æ¨ç† API

### 4.1 RESTful æ¨ç† API

**RESTful æ¨ç†ç«¯ç‚¹**ï¼š

```yaml
apiVersion: api.example.com/v1
kind: APIDefinition
metadata:
  name: ml-inference-api
spec:
  paths:
    /api/v1/models/{model_name}/predict:
      post:
        summary: Model prediction
        parameters:
          - name: model_name
            in: path
            required: true
            schema:
              type: string
        requestBody:
          content:
            application/json:
              schema:
                type: object
                properties:
                  inputs:
                    type: array
                    items:
                      type: number
```

### 4.2 gRPC æ¨ç† API

**gRPC æ¨ç†æœåŠ¡**ï¼š

```protobuf
syntax = "proto3";

package ml.v1;

service InferenceService {
  rpc Predict(PredictRequest) returns (PredictResponse);
  rpc BatchPredict(BatchPredictRequest) returns (BatchPredictResponse);
}

message PredictRequest {
  string model_name = 1;
  repeated float inputs = 2;
}

message PredictResponse {
  repeated float outputs = 1;
  float latency_ms = 2;
}
```

---

## 5. æ¨¡å‹ç®¡ç†

### 5.1 æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

**æ¨¡å‹ç‰ˆæœ¬é…ç½®**ï¼š

```yaml
apiVersion: ml.example.com/v1
kind: ModelVersion
metadata:
  name: payment-model-v1
spec:
  modelName: payment-model
  version: "1.0.0"
  format: onnx
  storage:
    type: s3
    path: s3://models/payment-model-v1.onnx
```

### 5.2 A/B æµ‹è¯•

**A/B æµ‹è¯•é…ç½®**ï¼š

```yaml
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: payment-model-ab
spec:
  traffic:
    - revisionName: payment-model-v1
      percent: 50
    - revisionName: payment-model-v2
      percent: 50
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 æ‰¹å¤„ç†ä¼˜åŒ–

**æ‰¹å¤„ç†é…ç½®**ï¼š

```yaml
apiVersion: ml.example.com/v1
kind: InferenceConfig
metadata:
  name: payment-model-config
spec:
  batchSize: 32
  maxWaitTime: "100ms"
  timeout: "1s"
```

### 6.2 æ¨¡å‹é‡åŒ–

**æ¨¡å‹é‡åŒ–é…ç½®**ï¼š

```yaml
apiVersion: ml.example.com/v1
kind: ModelOptimization
metadata:
  name: payment-model-quantization
spec:
  quantization:
    enabled: true
    precision: int8
    target: cpu
```

---

## 7. ç›¸å…³æ–‡æ¡£

- **[WASM åŒ– API è§„èŒƒ](../03-wasm-api/wasm-api.md)** - WASI-NN æ¥å£
- **[API æ€§èƒ½ä¼˜åŒ–](../14-api-performance/api-performance.md)** - ML æ€§èƒ½ä¼˜åŒ–
- **[API æ— æœåŠ¡å™¨æ¶æ„](../37-api-serverless/api-serverless.md)** - ML æ— æœåŠ¡å™¨
- **[æœ€ä½³å®è·µ](../08-best-practices/best-practices.md)** - AI/ML API æœ€ä½³å®è·µ
- **[API è§†è§’ä¸»æ–‡æ¡£](../../../api_view.md)** â­ - API è§„èŒƒè§†è§’çš„æ ¸å¿ƒè®ºè¿°

---

**æœ€åæ›´æ–°**ï¼š2025-11-07 **ç»´æŠ¤è€…**ï¼šé¡¹ç›®å›¢é˜Ÿ
