# AI/ML æ¶æ„è§†è§’

## ğŸ“‘ ç›®å½•

- [ğŸ“‘ ç›®å½•](#-ç›®å½•)
- [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [1.1 æ ¸å¿ƒæ€æƒ³](#11-æ ¸å¿ƒæ€æƒ³)
- [2. AI/ML åœ¨æ¶æ„ä¸­çš„å®šä½](#2-aiml-åœ¨æ¶æ„ä¸­çš„å®šä½)
  - [2.1 åœ¨ç»Ÿä¸€ä¸­å±‚æ¨¡å‹ä¸­çš„ä½ç½®](#21-åœ¨ç»Ÿä¸€ä¸­å±‚æ¨¡å‹ä¸­çš„ä½ç½®)
  - [2.2 ä¸å››å±‚æŠ½è±¡çš„å…³ç³»](#22-ä¸å››å±‚æŠ½è±¡çš„å…³ç³»)
- [3. AI/ML å·¥ä½œè´Ÿè½½ç‰¹ç‚¹](#3-aiml-å·¥ä½œè´Ÿè½½ç‰¹ç‚¹)
  - [3.1 èµ„æºå¯†é›†å‹](#31-èµ„æºå¯†é›†å‹)
  - [3.2 å¼‚æ„è®¡ç®—éœ€æ±‚](#32-å¼‚æ„è®¡ç®—éœ€æ±‚)
  - [3.3 æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†](#33-æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†)
- [4. LLM æ¨ç†ä¸å®¹å™¨ç¼–æ’é›†æˆ](#4-llm-æ¨ç†ä¸å®¹å™¨ç¼–æ’é›†æˆ)
  - [4.1 LLM æ¨ç†æ¶æ„](#41-llm-æ¨ç†æ¶æ„)
  - [4.2 å®¹å™¨ç¼–æ’ç­–ç•¥](#42-å®¹å™¨ç¼–æ’ç­–ç•¥)
  - [4.3 GPU èµ„æºè°ƒåº¦](#43-gpu-èµ„æºè°ƒåº¦)
- [5. MLOps ä¸ GitOps èåˆ](#5-mlops-ä¸-gitops-èåˆ)
  - [5.1 MLOps æµç¨‹](#51-mlops-æµç¨‹)
  - [5.2 GitOps é›†æˆ](#52-gitops-é›†æˆ)
  - [5.3 æ¨¡å‹ç‰ˆæœ¬ç®¡ç†](#53-æ¨¡å‹ç‰ˆæœ¬ç®¡ç†)
- [6. AI/ML æŠ€æœ¯æ ˆ](#6-aiml-æŠ€æœ¯æ ˆ)
  - [6.1 è®­ç»ƒæ¡†æ¶](#61-è®­ç»ƒæ¡†æ¶)
  - [6.2 æ¨ç†è¿è¡Œæ—¶](#62-æ¨ç†è¿è¡Œæ—¶)
  - [6.3 æ¨¡å‹æœåŠ¡](#63-æ¨¡å‹æœåŠ¡)
- [7. äº‘åŸç”Ÿ AI/ML å¹³å°](#7-äº‘åŸç”Ÿ-aiml-å¹³å°)
  - [7.1 Kubeflow](#71-kubeflow)
  - [7.2 MLflow](#72-mlflow)
  - [7.3 Seldon Core](#73-seldon-core)
- [8. è¾¹ç¼˜ AI æ¨ç†](#8-è¾¹ç¼˜-ai-æ¨ç†)
  - [8.1 è¾¹ç¼˜ AI æ¶æ„](#81-è¾¹ç¼˜-ai-æ¶æ„)
  - [8.2 WasmEdge AI](#82-wasmedge-ai)
  - [8.3 æ¨¡å‹ä¼˜åŒ–](#83-æ¨¡å‹ä¼˜åŒ–)
- [9. æœ€ä½³å®è·µ](#9-æœ€ä½³å®è·µ)
  - [9.1 èµ„æºç®¡ç†](#91-èµ„æºç®¡ç†)
  - [9.2 æ€§èƒ½ä¼˜åŒ–](#92-æ€§èƒ½ä¼˜åŒ–)
  - [9.3 æˆæœ¬ä¼˜åŒ–](#93-æˆæœ¬ä¼˜åŒ–)
- [10. ç›¸å…³æ–‡æ¡£](#10-ç›¸å…³æ–‡æ¡£)
  - [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)
  - [ç†è®ºæ–‡æ¡£](#ç†è®ºæ–‡æ¡£)
  - [å­¦æœ¯èµ„æº](#å­¦æœ¯èµ„æº)

---

## 1. æ¦‚è¿°

æœ¬æ–‡æ¡£ä»**AI/ML**è§†è§’é˜è¿°æ¶æ„è®¾è®¡ï¼Œè¯´æ˜ AI/ML å·¥ä½œè´Ÿè½½å¦‚ä½•ä¸äº‘åŸç”Ÿæ¶æ„é›†æˆï¼Œå®ç°
é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒã€æ¨ç†å’Œéƒ¨ç½²ã€‚

### 1.1 æ ¸å¿ƒæ€æƒ³

> **AI/ML å·¥ä½œè´Ÿè½½é€šè¿‡äº‘åŸç”Ÿæ¶æ„å®ç°èµ„æºçš„ç»Ÿä¸€ç®¡ç†ã€æ¨¡å‹çš„ç‰ˆæœ¬åŒ–éƒ¨ç½²å’Œæ¨ç†çš„å¼¹æ€§
> æ‰©ç¼©å®¹ï¼Œå®ç°è®­ç»ƒä¸æ¨ç†çš„åˆ†ç¦»ã€GPU èµ„æºçš„åŠ¨æ€è°ƒåº¦å’Œæ¨¡å‹çš„ç»Ÿä¸€æ²»ç†**

---

## 2. AI/ML åœ¨æ¶æ„ä¸­çš„å®šä½

### 2.1 åœ¨ç»Ÿä¸€ä¸­å±‚æ¨¡å‹ä¸­çš„ä½ç½®

**ç»Ÿä¸€ä¸­å±‚æ¨¡å‹ â„³**ï¼šâ„³ â‰œ âŸ¨U, G, P, Î”âŸ©

**AI/ML å·¥ä½œè´Ÿè½½æ˜ å°„**ï¼š

- **Uï¼ˆè®¡ç®—å•å…ƒï¼‰**ï¼š
  - **è®­ç»ƒ**ï¼šVM / Containerï¼ˆGPU èŠ‚ç‚¹ï¼‰
  - **æ¨ç†**ï¼šContainer / Sandbox / Wasmï¼ˆCPU/GPU èŠ‚ç‚¹ï¼‰
- **Gï¼ˆç»„åˆå›¾è°±ï¼‰**ï¼š
  - **æ¨¡å‹æœåŠ¡**ï¼šLLM æœåŠ¡ã€æ¨ç†æœåŠ¡ã€ç‰¹å¾æœåŠ¡
  - **æ•°æ®ç®¡é“**ï¼šæ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒ
- **Pï¼ˆç­–ç•¥å±‚ï¼‰**ï¼š
  - **èµ„æºç­–ç•¥**ï¼šGPU èµ„æºåˆ†é…ã€èŠ‚ç‚¹é€‰æ‹©
  - **æ‰©ç¼©å®¹ç­–ç•¥**ï¼šHPAã€VPAã€GPU è‡ªåŠ¨æ‰©ç¼©å®¹
  - **è´¨é‡ç­–ç•¥**ï¼šæ¨¡å‹ç‰ˆæœ¬ç®¡ç†ã€A/B æµ‹è¯•ã€é‡‘ä¸é›€å‘å¸ƒ

### 2.2 ä¸å››å±‚æŠ½è±¡çš„å…³ç³»

**AI/ML å·¥ä½œè´Ÿè½½åœ¨ä¸åŒæŠ½è±¡å±‚çš„åº”ç”¨**ï¼š

| æŠ½è±¡å±‚          | AI/ML åº”ç”¨åœºæ™¯         | å…¸å‹æŠ€æœ¯                              |
| --------------- | ---------------------- | ------------------------------------- |
| **è™šæ‹ŸåŒ–**      | GPU èµ„æºæ± åŒ–           | GPU è™šæ‹ŸåŒ–ã€MIGï¼ˆMulti-Instance GPUï¼‰ |
| **å®¹å™¨åŒ–**      | æ¨¡å‹è®­ç»ƒã€æ¨ç†æœåŠ¡     | Dockerã€Kubernetes GPU æ’ä»¶           |
| **æ²™ç›’åŒ–**      | è½»é‡æ¨ç†ã€æ¨¡å‹æœåŠ¡     | gVisorã€Firecracker                   |
| **WebAssembly** | è¾¹ç¼˜æ¨ç†ã€è½»é‡æ¨¡å‹éƒ¨ç½² | WasmEdge AIã€ONNX Runtime             |

---

## 3. AI/ML å·¥ä½œè´Ÿè½½ç‰¹ç‚¹

### 3.1 èµ„æºå¯†é›†å‹

**ç‰¹ç‚¹**ï¼š

- **GPU éœ€æ±‚**ï¼šè®­ç»ƒéœ€è¦å¤§é‡ GPU èµ„æº
- **å†…å­˜éœ€æ±‚**ï¼šå¤§æ¨¡å‹éœ€è¦å¤§å†…å­˜ï¼ˆæ•°ç™¾ GBï¼‰
- **å­˜å‚¨éœ€æ±‚**ï¼šæ¨¡å‹æ–‡ä»¶å’Œæ•°æ®éœ€è¦å¤§å­˜å‚¨

**èµ„æºç®¡ç†ç­–ç•¥**ï¼š

- **GPU èµ„æºæ± åŒ–**ï¼šé€šè¿‡ GPU è™šæ‹ŸåŒ–å®ç°èµ„æºæ± åŒ–
- **åŠ¨æ€è°ƒåº¦**ï¼šæ ¹æ®å·¥ä½œè´Ÿè½½éœ€æ±‚åŠ¨æ€åˆ†é… GPU
- **èµ„æºéš”ç¦»**ï¼šé€šè¿‡å®¹å™¨å’Œå‘½åç©ºé—´å®ç°èµ„æºéš”ç¦»

### 3.2 å¼‚æ„è®¡ç®—éœ€æ±‚

**è®¡ç®—ç±»å‹**ï¼š

- **è®­ç»ƒ**ï¼šéœ€è¦é«˜æ€§èƒ½ GPUï¼ˆNVIDIA A100ã€H100ï¼‰
- **æ¨ç†**ï¼šéœ€è¦ä½å»¶è¿Ÿ CPU/GPUï¼ˆNVIDIA T4ã€è¾¹ç¼˜ GPUï¼‰
- **é¢„å¤„ç†**ï¼šéœ€è¦ CPU å¯†é›†å‹è®¡ç®—

**å¼‚æ„è®¡ç®—æ¶æ„**ï¼š

```text
è®­ç»ƒé›†ç¾¤ï¼ˆGPU èŠ‚ç‚¹ï¼‰
â”œâ”€â”€ æ•°æ®é¢„å¤„ç†ï¼ˆCPUï¼‰
â”œâ”€â”€ æ¨¡å‹è®­ç»ƒï¼ˆGPUï¼‰
â””â”€â”€ æ¨¡å‹éªŒè¯ï¼ˆGPUï¼‰

æ¨ç†é›†ç¾¤ï¼ˆCPU/GPU æ··åˆï¼‰
â”œâ”€â”€ è¾¹ç¼˜æ¨ç†ï¼ˆCPU/Wasmï¼‰
â”œâ”€â”€ äº‘ç«¯æ¨ç†ï¼ˆGPUï¼‰
â””â”€â”€ æ‰¹é‡æ¨ç†ï¼ˆGPUï¼‰
```

### 3.3 æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†

**ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ**ï¼š

1. **è®­ç»ƒ**ï¼šæ•°æ®å‡†å¤‡ â†’ æ¨¡å‹è®­ç»ƒ â†’ æ¨¡å‹éªŒè¯
2. **éƒ¨ç½²**ï¼šæ¨¡å‹æ‰“åŒ… â†’ æ¨¡å‹æ³¨å†Œ â†’ æ¨¡å‹éƒ¨ç½²
3. **æ¨ç†**ï¼šè¯·æ±‚å¤„ç† â†’ æ¨¡å‹æ¨ç† â†’ ç»“æœè¿”å›
4. **ç›‘æ§**ï¼šæ€§èƒ½ç›‘æ§ â†’ è´¨é‡ç›‘æ§ â†’ ç‰ˆæœ¬ç®¡ç†
5. **æ›´æ–°**ï¼šA/B æµ‹è¯• â†’ é‡‘ä¸é›€å‘å¸ƒ â†’ ç‰ˆæœ¬åˆ‡æ¢

---

## 4. LLM æ¨ç†ä¸å®¹å™¨ç¼–æ’é›†æˆ

### 4.1 LLM æ¨ç†æ¶æ„

**æ¶æ„è®¾è®¡**ï¼š

```text
LLM æ¨ç†æœåŠ¡
â”œâ”€â”€ è¯·æ±‚å…¥å£ï¼ˆAPI Gatewayï¼‰
â”œâ”€â”€ è´Ÿè½½å‡è¡¡ï¼ˆService Meshï¼‰
â”œâ”€â”€ æ¨ç†æœåŠ¡ï¼ˆKubernetes Podï¼‰
â”‚   â”œâ”€â”€ æ¨¡å‹åŠ è½½ï¼ˆGPU å†…å­˜ï¼‰
â”‚   â”œâ”€â”€ æ¨ç†æ‰§è¡Œï¼ˆGPU è®¡ç®—ï¼‰
â”‚   â””â”€â”€ ç»“æœè¿”å›ï¼ˆç½‘ç»œï¼‰
â””â”€â”€ ç›‘æ§å’Œæ—¥å¿—ï¼ˆPrometheus + Grafanaï¼‰
```

**å…³é”®ç»„ä»¶**ï¼š

- **æ¨¡å‹æœåŠ¡**ï¼šä½¿ç”¨ Seldon Coreã€KServe ç­‰æ¨¡å‹æœåŠ¡æ¡†æ¶
- **GPU è°ƒåº¦**ï¼šä½¿ç”¨ Kubernetes GPU æ’ä»¶ï¼ˆNVIDIA GPU Operatorï¼‰
- **æµé‡ç®¡ç†**ï¼šä½¿ç”¨ Istio/Linkerd è¿›è¡Œæµé‡ç®¡ç†å’Œé‡‘ä¸é›€å‘å¸ƒ

### 4.2 å®¹å™¨ç¼–æ’ç­–ç•¥

**Kubernetes éƒ¨ç½²**ï¼š

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: llm-inference
spec:
  containers:
    - name: llm-service
      image: my-registry/llm-service:v1.0.0
      resources:
        limits:
          nvidia.com/gpu: 1
          memory: "32Gi"
          cpu: "8"
        requests:
          nvidia.com/gpu: 1
          memory: "32Gi"
          cpu: "8"
      env:
        - name: MODEL_PATH
          value: "/models/llm-v1.wasm"
```

**GPU èµ„æºç®¡ç†**ï¼š

- **GPU å…±äº«**ï¼šä½¿ç”¨ MIGï¼ˆMulti-Instance GPUï¼‰å®ç° GPU å…±äº«
- **GPU è°ƒåº¦**ï¼šä½¿ç”¨ Kubernetes GPU è°ƒåº¦å™¨å®ç° GPU èµ„æºè°ƒåº¦
- **GPU ç›‘æ§**ï¼šä½¿ç”¨ DCGMï¼ˆNVIDIA Data Center GPU Managerï¼‰ç›‘æ§ GPU ä½¿ç”¨

### 4.3 GPU èµ„æºè°ƒåº¦

**GPU è°ƒåº¦ç­–ç•¥**ï¼š

1. **é™æ€åˆ†é…**ï¼šPod ç‹¬å  GPU
2. **å…±äº«åˆ†é…**ï¼šå¤šä¸ª Pod å…±äº« GPUï¼ˆä½¿ç”¨ MIGï¼‰
3. **åŠ¨æ€åˆ†é…**ï¼šæ ¹æ®å·¥ä½œè´Ÿè½½åŠ¨æ€åˆ†é… GPU

**GPU è°ƒåº¦å™¨é…ç½®**ï¼š

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-scheduler-config
data:
  config.yaml: |
    scheduler:
      gpuAllocationPolicy: "shared"  # shared, exclusive, dynamic
      gpuSharingStrategy: "time-slicing"  # time-slicing, mps
      maxGPUsPerPod: 8
```

---

## 5. MLOps ä¸ GitOps èåˆ

### 5.1 MLOps æµç¨‹

**MLOps æµç¨‹**ï¼š

```text
æ•°æ®å‡†å¤‡
  â†“
æ¨¡å‹è®­ç»ƒ
  â†“
æ¨¡å‹éªŒè¯
  â†“
æ¨¡å‹æ³¨å†Œï¼ˆMLflowï¼‰
  â†“
æ¨¡å‹éƒ¨ç½²ï¼ˆGitOpsï¼‰
  â†“
æ¨¡å‹ç›‘æ§
  â†“
æ¨¡å‹æ›´æ–°
```

**å…³é”®æ­¥éª¤**ï¼š

- **è®­ç»ƒ**ï¼šåœ¨ GPU é›†ç¾¤ä¸Šè¿›è¡Œæ¨¡å‹è®­ç»ƒ
- **æ³¨å†Œ**ï¼šå°†è®­ç»ƒå¥½çš„æ¨¡å‹æ³¨å†Œåˆ° MLflow Model Registry
- **éƒ¨ç½²**ï¼šé€šè¿‡ GitOpsï¼ˆArgoCDï¼‰è‡ªåŠ¨éƒ¨ç½²æ¨¡å‹
- **ç›‘æ§**ï¼šç›‘æ§æ¨¡å‹æ€§èƒ½å’Œæ¨ç†è´¨é‡

### 5.2 GitOps é›†æˆ

**GitOps å·¥ä½œæµ**ï¼š

```text
æ¨¡å‹æ³¨å†Œï¼ˆMLflowï¼‰
  â†“
è§¦å‘ GitOps æ›´æ–°ï¼ˆWebhookï¼‰
  â†“
æ›´æ–° Kubernetes é…ç½®ï¼ˆGitï¼‰
  â†“
ArgoCD è‡ªåŠ¨åŒæ­¥
  â†“
éƒ¨ç½²æ–°æ¨¡å‹ç‰ˆæœ¬
  â†“
é‡‘ä¸é›€å‘å¸ƒï¼ˆIstioï¼‰
  â†“
å…¨é‡å‘å¸ƒ
```

**ArgoCD é…ç½®**ï¼š

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llm-inference
spec:
  source:
    repoURL: https://github.com/myorg/ml-models
    path: deployments/llm-inference
    targetRevision: main
  destination:
    server: https://kubernetes.default.svc
    namespace: ml-production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

### 5.3 æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

**æ¨¡å‹ç‰ˆæœ¬ç­–ç•¥**ï¼š

- **è¯­ä¹‰åŒ–ç‰ˆæœ¬**ï¼šv1.0.0, v1.1.0, v2.0.0
- **Git æ ‡ç­¾**ï¼šä½¿ç”¨ Git æ ‡ç­¾æ ‡è®°æ¨¡å‹ç‰ˆæœ¬
- **æ¨¡å‹æ³¨å†Œè¡¨**ï¼šä½¿ç”¨ MLflow Model Registry ç®¡ç†æ¨¡å‹ç‰ˆæœ¬

**ç‰ˆæœ¬åˆ‡æ¢**ï¼š

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: llm-inference
spec:
  hosts:
    - llm-service
  http:
    - match:
        - headers:
            version:
              exact: "v1"
      route:
        - destination:
            host: llm-service
            subset: v1
          weight: 90
        - destination:
            host: llm-service
            subset: v2
          weight: 10
```

---

## 6. AI/ML æŠ€æœ¯æ ˆ

### 6.1 è®­ç»ƒæ¡†æ¶

**ä¸»æµæ¡†æ¶**ï¼š

- **PyTorch**ï¼šæ·±åº¦å­¦ä¹ è®­ç»ƒæ¡†æ¶
- **TensorFlow**ï¼šæ·±åº¦å­¦ä¹ è®­ç»ƒæ¡†æ¶
- **JAX**ï¼šé«˜æ€§èƒ½æœºå™¨å­¦ä¹ æ¡†æ¶
- **Hugging Face Transformers**ï¼šé¢„è®­ç»ƒæ¨¡å‹åº“

**å®¹å™¨åŒ–éƒ¨ç½²**ï¼š

```dockerfile
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY train.py .
COPY data/ ./data/

CMD ["python", "train.py"]
```

### 6.2 æ¨ç†è¿è¡Œæ—¶

**æ¨ç†è¿è¡Œæ—¶**ï¼š

- **ONNX Runtime**ï¼šè·¨å¹³å°æ¨ç†è¿è¡Œæ—¶
- **TensorRT**ï¼šNVIDIA GPU æ¨ç†ä¼˜åŒ–
- **WasmEdge AI**ï¼šWebAssembly AI æ¨ç†è¿è¡Œæ—¶
- **TorchServe**ï¼šPyTorch æ¨¡å‹æœåŠ¡

**WasmEdge AI ç¤ºä¾‹**ï¼š

```bash
# ç¼–è¯‘ ONNX æ¨¡å‹ä¸º Wasm
wasmedge compile --enable-gpu model.onnx model.wasm

# è¿è¡Œ Wasm æ¨¡å‹
wasmedge --enable-gpu model.wasm
```

### 6.3 æ¨¡å‹æœåŠ¡

**æ¨¡å‹æœåŠ¡æ¡†æ¶**ï¼š

- **KServe**ï¼šKubernetes åŸç”Ÿæ¨¡å‹æœåŠ¡æ¡†æ¶
- **Seldon Core**ï¼šæœºå™¨å­¦ä¹ éƒ¨ç½²å¹³å°
- **Triton Inference Server**ï¼šNVIDIA æ¨ç†æœåŠ¡å™¨

**KServe éƒ¨ç½²ç¤ºä¾‹**ï¼š

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llm-service
spec:
  predictor:
    containers:
      - name: llm
        image: my-registry/llm-service:v1.0.0
        resources:
          limits:
            nvidia.com/gpu: 1
```

---

## 7. äº‘åŸç”Ÿ AI/ML å¹³å°

### 7.1 Kubeflow

**Kubeflow ç»„ä»¶**ï¼š

- **Kubeflow Pipelines**ï¼šæœºå™¨å­¦ä¹ å·¥ä½œæµç¼–æ’
- **Katib**ï¼šè‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜
- **KServe**ï¼šæ¨¡å‹æœåŠ¡æ¡†æ¶
- **Training Operator**ï¼šåˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

**Pipeline ç¤ºä¾‹**ï¼š

```python
from kfp import dsl

@dsl.pipeline(
    name='llm-training-pipeline',
    description='LLM training pipeline'
)
def llm_training_pipeline():
    preprocess = dsl.ContainerOp(
        name='preprocess',
        image='preprocess:latest',
        command=['python', 'preprocess.py']
    )

    train = dsl.ContainerOp(
        name='train',
        image='train:latest',
        command=['python', 'train.py']
    )

    train.after(preprocess)
```

### 7.2 MLflow

**MLflow ç»„ä»¶**ï¼š

- **MLflow Tracking**ï¼šå®éªŒè·Ÿè¸ª
- **MLflow Projects**ï¼šå¯é‡ç°çš„ä»£ç æ‰“åŒ…
- **MLflow Models**ï¼šæ¨¡å‹æ‰“åŒ…å’Œéƒ¨ç½²
- **MLflow Registry**ï¼šæ¨¡å‹æ³¨å†Œè¡¨

**æ¨¡å‹æ³¨å†Œ**ï¼š

```python
import mlflow

# è®­ç»ƒæ¨¡å‹
model = train_model()

# æ³¨å†Œæ¨¡å‹
mlflow.register_model(
    model_uri=f"runs:/{run_id}/model",
    name="llm-model"
)
```

### 7.3 Seldon Core

**Seldon Core ç‰¹æ€§**ï¼š

- **å¤šæ¡†æ¶æ”¯æŒ**ï¼šæ”¯æŒ TensorFlowã€PyTorchã€Scikit-learn ç­‰
- **A/B æµ‹è¯•**ï¼šæ”¯æŒæ¨¡å‹ A/B æµ‹è¯•
- **å¤šè‡‚è€è™æœº**ï¼šæ”¯æŒå¤šè‡‚è€è™æœºç®—æ³•
- **å¯è§£é‡Šæ€§**ï¼šæ”¯æŒæ¨¡å‹å¯è§£é‡Šæ€§

**Seldon éƒ¨ç½²**ï¼š

```yaml
apiVersion: machinelearning.seldon.io/v1
kind: SeldonDeployment
metadata:
  name: llm-model
spec:
  predictors:
    - name: default
      graph:
        name: llm-model
        type: MODEL
        modelUri: s3://models/llm-v1
      replicas: 3
      componentSpecs:
        - spec:
            containers:
              - name: llm-model
                image: my-registry/llm-service:v1.0.0
```

---

## 8. è¾¹ç¼˜ AI æ¨ç†

### 8.1 è¾¹ç¼˜ AI æ¶æ„

**æ¶æ„è®¾è®¡**ï¼š

```text
äº‘ç«¯è®­ç»ƒ
  â†“
æ¨¡å‹ä¼˜åŒ–ï¼ˆé‡åŒ–ã€å‰ªæï¼‰
  â†“
æ¨¡å‹ç¼–è¯‘ï¼ˆONNX â†’ Wasmï¼‰
  â†“
è¾¹ç¼˜éƒ¨ç½²ï¼ˆK3s + WasmEdgeï¼‰
  â†“
è¾¹ç¼˜æ¨ç†ï¼ˆä½å»¶è¿Ÿï¼‰
  â†“
ç»“æœä¸ŠæŠ¥ï¼ˆäº‘ç«¯ï¼‰
```

**è¾¹ç¼˜ AI ä¼˜åŠ¿**ï¼š

- **ä½å»¶è¿Ÿ**ï¼šå°±è¿‘æ¨ç†ï¼Œé™ä½å»¶è¿Ÿ
- **éšç§ä¿æŠ¤**ï¼šæ•°æ®ä¸å‡ºè¾¹ç¼˜ï¼Œä¿æŠ¤éšç§
- **ç¦»çº¿æ”¯æŒ**ï¼šæ”¯æŒç¦»çº¿æ¨ç†

### 8.2 WasmEdge AI

**WasmEdge AI ç‰¹æ€§**ï¼š

- **è½»é‡çº§**ï¼šè¿è¡Œæ—¶ < 10 MB
- **å¿«é€Ÿå¯åŠ¨**ï¼šå†·å¯åŠ¨ < 1ms
- **GPU åŠ é€Ÿ**ï¼šæ”¯æŒ GPU åŠ é€Ÿæ¨ç†
- **å¤šæ¡†æ¶æ”¯æŒ**ï¼šæ”¯æŒ ONNXã€TensorFlowã€PyTorch

**WasmEdge AI ç¤ºä¾‹**ï¼š

```bash
# å®‰è£… WasmEdge AI æ’ä»¶
wasmedge install tensorflow

# è¿è¡Œ AI æ¨ç†
wasmedge --enable-gpu tensorflow.wasm input.jpg
```

### 8.3 æ¨¡å‹ä¼˜åŒ–

**ä¼˜åŒ–æŠ€æœ¯**ï¼š

- **é‡åŒ–**ï¼šINT8 é‡åŒ–ï¼Œå‡å°‘æ¨¡å‹å¤§å°
- **å‰ªæ**ï¼šç§»é™¤å†—ä½™å‚æ•°
- **è’¸é¦**ï¼šçŸ¥è¯†è’¸é¦ï¼Œå‡å°æ¨¡å‹
- **ç¼–è¯‘ä¼˜åŒ–**ï¼šONNX Runtime ä¼˜åŒ–

**ä¼˜åŒ–æ•ˆæœ**ï¼š

- **æ¨¡å‹å¤§å°**ï¼šå‡å°‘ 70-90%
- **æ¨ç†é€Ÿåº¦**ï¼šæå‡ 2-5Ã—
- **å†…å­˜å ç”¨**ï¼šå‡å°‘ 50-80%

---

## 9. æœ€ä½³å®è·µ

### 9.1 èµ„æºç®¡ç†

**GPU èµ„æºç®¡ç†**ï¼š

- **èµ„æºæ± åŒ–**ï¼šä½¿ç”¨ GPU è™šæ‹ŸåŒ–å®ç°èµ„æºæ± åŒ–
- **åŠ¨æ€è°ƒåº¦**ï¼šæ ¹æ®å·¥ä½œè´Ÿè½½åŠ¨æ€åˆ†é… GPU
- **èµ„æºéš”ç¦»**ï¼šä½¿ç”¨å®¹å™¨å’Œå‘½åç©ºé—´å®ç°èµ„æºéš”ç¦»

**CPU/å†…å­˜èµ„æºç®¡ç†**ï¼š

- **HPA**ï¼šæ ¹æ® CPU/å†…å­˜ä½¿ç”¨ç‡è‡ªåŠ¨æ‰©ç¼©å®¹
- **VPA**ï¼šæ ¹æ®å†å²æ•°æ®æ¨èèµ„æºè¯·æ±‚
- **èµ„æºé™åˆ¶**ï¼šè®¾ç½®åˆç†çš„èµ„æºé™åˆ¶

### 9.2 æ€§èƒ½ä¼˜åŒ–

**æ¨ç†æ€§èƒ½ä¼˜åŒ–**ï¼š

- **æ‰¹å¤„ç†**ï¼šæ‰¹é‡å¤„ç†è¯·æ±‚ï¼Œæå‡ååé‡
- **æ¨¡å‹ç¼“å­˜**ï¼šç¼“å­˜æ¨¡å‹åˆ° GPU å†…å­˜
- **å¼‚æ­¥æ¨ç†**ï¼šå¼‚æ­¥å¤„ç†è¯·æ±‚ï¼Œæå‡å¹¶å‘

**è®­ç»ƒæ€§èƒ½ä¼˜åŒ–**ï¼š

- **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šä½¿ç”¨ Horovodã€PyTorch DDP ç­‰
- **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šä½¿ç”¨ FP16/BF16 æå‡è®­ç»ƒé€Ÿåº¦
- **æ•°æ®å¹¶è¡Œ**ï¼šæ•°æ®å¹¶è¡Œå¤„ç†ï¼Œæå‡è®­ç»ƒæ•ˆç‡

### 9.3 æˆæœ¬ä¼˜åŒ–

**æˆæœ¬ä¼˜åŒ–ç­–ç•¥**ï¼š

- **Spot å®ä¾‹**ï¼šä½¿ç”¨ Spot å®ä¾‹è¿›è¡Œè®­ç»ƒ
- **è‡ªåŠ¨æ‰©ç¼©å®¹**ï¼šæ ¹æ®å·¥ä½œè´Ÿè½½è‡ªåŠ¨æ‰©ç¼©å®¹
- **æ¨¡å‹å‹ç¼©**ï¼šä½¿ç”¨æ¨¡å‹å‹ç¼©æŠ€æœ¯å‡å°‘èµ„æºéœ€æ±‚
- **è¾¹ç¼˜æ¨ç†**ï¼šå°†æ¨ç†ä¸‹æ²‰åˆ°è¾¹ç¼˜ï¼Œå‡å°‘äº‘ç«¯æˆæœ¬

---

## 10. ç›¸å…³æ–‡æ¡£

### ç›¸å…³æ–‡æ¡£

- **[WebAssembly è§†è§’](webassembly-view.md)** - WebAssembly æ¶æ„è§†è§’ï¼ˆè¾¹ç¼˜ AI æ¨
  ç†ï¼‰
- **[è¾¹ç¼˜è®¡ç®—è§†è§’](edge-computing-view.md)** - è¾¹ç¼˜è®¡ç®—æ¶æ„è§†è§’ï¼ˆå¾…åˆ›å»ºï¼‰
- **[åŠ¨æ€è¿ç»´è§†è§’](dynamic-operations-view.md)** - åŠ¨æ€è¿ç»´æ¶æ„è§†è§’
  ï¼ˆMLOpsã€GitOpsï¼‰

### ç†è®ºæ–‡æ¡£

- **[Î¨â‚…ï¼šç¬¬äº”æ¬¡å½’çº³æ˜ å°„](../00-theory/02-induction-proof/psi5-wasm.md)** -
  WebAssembly æŠ½è±¡å±‚ï¼ˆAI æ¨ç†ï¼‰
- **[L4ï¼šWasm å†…å­˜å®‰å…¨å¼•ç†](../00-theory/05-lemmas-theorems/L4-wasm-memory-safety.md)** -
  å†…å­˜å®‰å…¨ï¼ˆAI æ¨ç†ï¼‰

### å­¦æœ¯èµ„æº

- **[ACADEMIC-REFERENCES.md](../ACADEMIC-REFERENCES.md)** - å­¦æœ¯èµ„æºæ–‡æ¡£
  - **Stanford CS 329S**ï¼šMachine Learning Systems Design
  - **Wikipedia**ï¼šMachine Learningã€Deep Learning

---

**æ›´æ–°æ—¶é—´**ï¼š2025-11-05 **ç‰ˆæœ¬**ï¼šv1.0 **å‚è€ƒ**ï¼š`architecture_view.md` AI/ML
æ¶æ„éƒ¨åˆ†
