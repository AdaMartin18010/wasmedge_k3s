# KServe æ¨¡å‹éƒ¨ç½²

## ğŸ“‘ ç›®å½•

- [1. æ¦‚è¿°](#1-æ¦‚è¿°)
- [2. KServe å®‰è£…](#2-kserve-å®‰è£…)
- [3. æ¨¡å‹éƒ¨ç½²](#3-æ¨¡å‹éƒ¨ç½²)
- [4. é‡‘ä¸é›€å‘å¸ƒ](#4-é‡‘ä¸é›€å‘å¸ƒ)
- [5. ç›¸å…³æ–‡æ¡£](#5-ç›¸å…³æ–‡æ¡£)

---

## 1. æ¦‚è¿°

**KServe** æ˜¯ Kubernetes åŸç”Ÿæ¨¡å‹æœåŠ¡æ¡†æ¶ï¼Œæä¾›æ¨¡å‹éƒ¨ç½²ã€è‡ªåŠ¨æ‰©ç¼©å®¹ã€é‡‘ä¸é›€å‘å¸ƒç­‰åŠŸèƒ½ã€‚

### 1.1 æ ¸å¿ƒç‰¹æ€§

- **å¤šæ¡†æ¶æ”¯æŒ**ï¼šTensorFlowã€PyTorchã€Scikit-learnã€ONNXã€XGBoost
- **è‡ªåŠ¨æ‰©ç¼©å®¹**ï¼šåŸºäºè¯·æ±‚é‡è‡ªåŠ¨æ‰©ç¼©å®¹
- **é‡‘ä¸é›€å‘å¸ƒ**ï¼šæ”¯æŒæ¨¡å‹ç‰ˆæœ¬çš„é‡‘ä¸é›€å‘å¸ƒ
- **æ¨ç†å›¾**ï¼šæ”¯æŒå¤æ‚çš„æ¨ç†å›¾ï¼ˆé¢„å¤„ç†ã€æ¨ç†ã€åå¤„ç†ï¼‰

---

## 2. KServe å®‰è£…

### 2.1 å‰ç½®è¦æ±‚

- **Kubernetes**ï¼šâ‰¥ 1.28
- **Istio**ï¼šâ‰¥ 1.21ï¼ˆå¯é€‰ï¼Œç”¨äºæµé‡ç®¡ç†ï¼‰

### 2.2 å®‰è£…æ­¥éª¤

```bash
# å®‰è£… KServe
kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.11.0/kserve.yaml

# å®‰è£… KServe è¿è¡Œæ—¶ï¼ˆä»¥ PyTorch ä¸ºä¾‹ï¼‰
kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.11.0/pytorch.yaml
```

### 2.3 éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥ KServe æ§åˆ¶å™¨
kubectl get pods -n kserve-system

# æ£€æŸ¥ KServe CRD
kubectl get crd | grep kserve
```

---

## 3. æ¨¡å‹éƒ¨ç½²

### 3.1 ç®€å•éƒ¨ç½²ï¼ˆTensorFlowï¼‰

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: tensorflow-model
spec:
  predictor:
    tensorflow:
      storageUri: s3://models/tensorflow-model/
      resources:
        limits:
          memory: "4Gi"
          cpu: "2"
        requests:
          memory: "2Gi"
          cpu: "1"
```

### 3.2 GPU éƒ¨ç½²ï¼ˆPyTorchï¼‰

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: pytorch-model
spec:
  predictor:
    pytorch:
      storageUri: s3://models/pytorch-model/
      resources:
        limits:
          nvidia.com/gpu: 1
          memory: "32Gi"
          cpu: "8"
        requests:
          nvidia.com/gpu: 1
          memory: "32Gi"
          cpu: "8"
```

### 3.3 è‡ªå®šä¹‰æ¨ç†æœåŠ¡

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: custom-model
spec:
  predictor:
    containers:
      - name: custom-inference
        image: my-registry/custom-inference:v1.0.0
        resources:
          limits:
            memory: "4Gi"
            cpu: "2"
```

---

## 4. é‡‘ä¸é›€å‘å¸ƒ

### 4.1 å¤šç‰ˆæœ¬éƒ¨ç½²

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llm-model
spec:
  predictor:
    canaryTrafficPercent: 10  # 10% æµé‡åˆ°æ–°ç‰ˆæœ¬
    pytorch:
      storageUri: s3://models/llm-model-v2/
      resources:
        limits:
          nvidia.com/gpu: 1
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llm-model-canary
spec:
  predictor:
    pytorch:
      storageUri: s3://models/llm-model-v2/
      resources:
        limits:
          nvidia.com/gpu: 1
```

### 4.2 Istio æµé‡ç®¡ç†

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: llm-model
spec:
  hosts:
    - llm-model
  http:
    - match:
        - headers:
            version:
              exact: "v2"
      route:
        - destination:
            host: llm-model-canary
            weight: 100
    - route:
        - destination:
            host: llm-model
            weight: 90
        - destination:
            host: llm-model-canary
            weight: 10
```

---

## 5. ç›¸å…³æ–‡æ¡£

- [`README.md`](README.md) - AI/ML å®ç°ç»†èŠ‚æ€»è§ˆ
- [`kubeflow-setup.md`](kubeflow-setup.md) - Kubeflow å®‰è£…å’Œé…ç½®
- [`mlflow-integration.md`](mlflow-integration.md) - MLflow é›†æˆå’Œé…ç½®

---

**æ›´æ–°æ—¶é—´**ï¼š2025-11-05 **ç‰ˆæœ¬**ï¼šv1.0

