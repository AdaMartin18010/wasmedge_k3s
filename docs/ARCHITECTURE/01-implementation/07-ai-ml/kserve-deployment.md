# KServe æ¨¡å‹éƒ¨ç½²

## ğŸ“‘ ç›®å½•

- [KServe æ¨¡å‹éƒ¨ç½²](#kserve-æ¨¡å‹éƒ¨ç½²)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1 æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1 æ ¸å¿ƒç‰¹æ€§](#11-æ ¸å¿ƒç‰¹æ€§)
  - [2 KServe å®‰è£…](#2-kserve-å®‰è£…)
    - [2.1 å‰ç½®è¦æ±‚](#21-å‰ç½®è¦æ±‚)
    - [2.2 å®‰è£…æ­¥éª¤](#22-å®‰è£…æ­¥éª¤)
    - [2.3 éªŒè¯å®‰è£…](#23-éªŒè¯å®‰è£…)
  - [3 æ¨¡å‹éƒ¨ç½²](#3-æ¨¡å‹éƒ¨ç½²)
    - [3.1 ç®€å•éƒ¨ç½²ï¼ˆTensorFlowï¼‰](#31-ç®€å•éƒ¨ç½²tensorflow)
    - [3.2 GPU éƒ¨ç½²ï¼ˆPyTorchï¼‰](#32-gpu-éƒ¨ç½²pytorch)
    - [3.3 è‡ªå®šä¹‰æ¨ç†æœåŠ¡](#33-è‡ªå®šä¹‰æ¨ç†æœåŠ¡)
  - [4 é‡‘ä¸é›€å‘å¸ƒ](#4-é‡‘ä¸é›€å‘å¸ƒ)
    - [4.1 å¤šç‰ˆæœ¬éƒ¨ç½²](#41-å¤šç‰ˆæœ¬éƒ¨ç½²)
    - [4.2 Istio æµé‡ç®¡ç†](#42-istio-æµé‡ç®¡ç†)
  - [5 ç›¸å…³æ–‡æ¡£](#5-ç›¸å…³æ–‡æ¡£)
  - [6 2025 å¹´æœ€æ–°å®è·µ](#6-2025-å¹´æœ€æ–°å®è·µ)
    - [6.1 KServe 0.12+ æ–°ç‰¹æ€§ï¼ˆ2025ï¼‰](#61-kserve-012-æ–°ç‰¹æ€§2025)
    - [6.2 è¾¹ç¼˜ KServe éƒ¨ç½²ï¼ˆ2025ï¼‰](#62-è¾¹ç¼˜-kserve-éƒ¨ç½²2025)
    - [6.3 Wasm æ¨¡å‹æ¨ç†ï¼ˆ2025ï¼‰](#63-wasm-æ¨¡å‹æ¨ç†2025)
  - [7 å®é™…åº”ç”¨æ¡ˆä¾‹](#7-å®é™…åº”ç”¨æ¡ˆä¾‹)
    - [æ¡ˆä¾‹ 1ï¼šå¤šæ¨¡å‹æœåŠ¡éƒ¨ç½²](#æ¡ˆä¾‹-1å¤šæ¨¡å‹æœåŠ¡éƒ¨ç½²)
    - [æ¡ˆä¾‹ 2ï¼šæ¨¡å‹é‡‘ä¸é›€å‘å¸ƒ](#æ¡ˆä¾‹-2æ¨¡å‹é‡‘ä¸é›€å‘å¸ƒ)
    - [æ¡ˆä¾‹ 3ï¼šè¾¹ç¼˜ AI æ¨ç†](#æ¡ˆä¾‹-3è¾¹ç¼˜-ai-æ¨ç†)

---

## 1 æ¦‚è¿°

**KServe** æ˜¯ Kubernetes åŸç”Ÿæ¨¡å‹æœåŠ¡æ¡†æ¶ï¼Œæä¾›æ¨¡å‹éƒ¨ç½²ã€è‡ªåŠ¨æ‰©ç¼©å®¹ã€é‡‘ä¸é›€å‘å¸ƒ
ç­‰åŠŸèƒ½ã€‚

### 1.1 æ ¸å¿ƒç‰¹æ€§

- **å¤šæ¡†æ¶æ”¯æŒ**ï¼šTensorFlowã€PyTorchã€Scikit-learnã€ONNXã€XGBoost
- **è‡ªåŠ¨æ‰©ç¼©å®¹**ï¼šåŸºäºè¯·æ±‚é‡è‡ªåŠ¨æ‰©ç¼©å®¹
- **é‡‘ä¸é›€å‘å¸ƒ**ï¼šæ”¯æŒæ¨¡å‹ç‰ˆæœ¬çš„é‡‘ä¸é›€å‘å¸ƒ
- **æ¨ç†å›¾**ï¼šæ”¯æŒå¤æ‚çš„æ¨ç†å›¾ï¼ˆé¢„å¤„ç†ã€æ¨ç†ã€åå¤„ç†ï¼‰

---

## 2 KServe å®‰è£…

### 2.1 å‰ç½®è¦æ±‚

- **Kubernetes**ï¼šâ‰¥ 1.28
- **Istio**ï¼šâ‰¥ 1.21ï¼ˆå¯é€‰ï¼Œç”¨äºæµé‡ç®¡ç†ï¼‰

### 2.2 å®‰è£…æ­¥éª¤

```bash
# å®‰è£… KServe
kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.11.0/kserve.yaml

# å®‰è£… KServe è¿è¡Œæ—¶ï¼ˆä»¥ PyTorch ä¸ºä¾‹ï¼‰
kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.11.0/pytorch.yaml
```

### 2.3 éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥ KServe æ§åˆ¶å™¨
kubectl get pods -n kserve-system

# æ£€æŸ¥ KServe CRD
kubectl get crd | grep kserve
```

---

## 3 æ¨¡å‹éƒ¨ç½²

### 3.1 ç®€å•éƒ¨ç½²ï¼ˆTensorFlowï¼‰

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: tensorflow-model
spec:
  predictor:
    tensorflow:
      storageUri: s3://models/tensorflow-model/
      resources:
        limits:
          memory: "4Gi"
          cpu: "2"
        requests:
          memory: "2Gi"
          cpu: "1"
```

### 3.2 GPU éƒ¨ç½²ï¼ˆPyTorchï¼‰

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: pytorch-model
spec:
  predictor:
    pytorch:
      storageUri: s3://models/pytorch-model/
      resources:
        limits:
          nvidia.com/gpu: 1
          memory: "32Gi"
          cpu: "8"
        requests:
          nvidia.com/gpu: 1
          memory: "32Gi"
          cpu: "8"
```

### 3.3 è‡ªå®šä¹‰æ¨ç†æœåŠ¡

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: custom-model
spec:
  predictor:
    containers:
      - name: custom-inference
        image: my-registry/custom-inference:v1.0.0
        resources:
          limits:
            memory: "4Gi"
            cpu: "2"
```

---

## 4 é‡‘ä¸é›€å‘å¸ƒ

### 4.1 å¤šç‰ˆæœ¬éƒ¨ç½²

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llm-model
spec:
  predictor:
    canaryTrafficPercent: 10 # 10% æµé‡åˆ°æ–°ç‰ˆæœ¬
    pytorch:
      storageUri: s3://models/llm-model-v2/
      resources:
        limits:
          nvidia.com/gpu: 1
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llm-model-canary
spec:
  predictor:
    pytorch:
      storageUri: s3://models/llm-model-v2/
      resources:
        limits:
          nvidia.com/gpu: 1
```

### 4.2 Istio æµé‡ç®¡ç†

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: llm-model
spec:
  hosts:
    - llm-model
  http:
    - match:
        - headers:
            version:
              exact: "v2"
      route:
        - destination:
            host: llm-model-canary
            weight: 100
    - route:
        - destination:
            host: llm-model
            weight: 90
        - destination:
            host: llm-model-canary
            weight: 10
```

---

## 5 ç›¸å…³æ–‡æ¡£

- [`README.md`](README.md) - AI/ML å®ç°ç»†èŠ‚æ€»è§ˆ
- [`kubeflow-setup.md`](kubeflow-setup.md) - Kubeflow å®‰è£…å’Œé…ç½®
- [`mlflow-integration.md`](mlflow-integration.md) - MLflow é›†æˆå’Œé…ç½®

## 6 2025 å¹´æœ€æ–°å®è·µ

### 6.1 KServe 0.12+ æ–°ç‰¹æ€§ï¼ˆ2025ï¼‰

**æœ€æ–°ç‰ˆæœ¬**ï¼šKServe 0.12+ï¼ˆ2025 å¹´ï¼‰

**æ–°ç‰¹æ€§**ï¼š

- **å¤šæ¨¡å‹æœåŠ¡**ï¼šæ”¯æŒå¤šæ¨¡å‹æœåŠ¡
- **è‡ªåŠ¨æ‰©ç¼©å®¹å¢å¼º**ï¼šæ”¹è¿›çš„è‡ªåŠ¨æ‰©ç¼©å®¹
- **æ€§èƒ½ä¼˜åŒ–**ï¼šæ¨ç†æ€§èƒ½æå‡ 30%

**å®‰è£…æœ€æ–°ç‰ˆæœ¬**ï¼š

```bash
# å®‰è£… KServe 0.12
kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.12.0/kserve.yaml
```

### 6.2 è¾¹ç¼˜ KServe éƒ¨ç½²ï¼ˆ2025ï¼‰

**2025 å¹´è¶‹åŠ¿**ï¼šåœ¨è¾¹ç¼˜èŠ‚ç‚¹éƒ¨ç½² KServe

**é…ç½®ç¤ºä¾‹**ï¼š

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: edge-model
spec:
  predictor:
    nodeSelector:
      node-type: edge
    containers:
    - name: kserve-container
      image: model:latest
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
        limits:
          cpu: "1"
          memory: "2Gi"
```

### 6.3 Wasm æ¨¡å‹æ¨ç†ï¼ˆ2025ï¼‰

**2025 å¹´è¶‹åŠ¿**ï¼šä½¿ç”¨ Wasm è¿è¡Œæ¨¡å‹æ¨ç†

**é…ç½®ç¤ºä¾‹**ï¼š

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: wasm-model
spec:
  predictor:
    runtimeClassName: wasm
    containers:
    - name: wasm-container
      image: wasm-model:latest
      resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
```

## 7 å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šå¤šæ¨¡å‹æœåŠ¡éƒ¨ç½²

**åœºæ™¯**ï¼šéƒ¨ç½²å¤šä¸ªæ¨¡å‹æœåŠ¡

**å®ç°æ–¹æ¡ˆ**ï¼š

```yaml
# æ¨¡å‹ A
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: model-a
spec:
  predictor:
    pytorch:
      storageUri: s3://models/model-a
---
# æ¨¡å‹ B
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: model-b
spec:
  predictor:
    tensorflow:
      storageUri: s3://models/model-b
```

**æ•ˆæœ**ï¼š

- å¤šæ¨¡å‹ï¼šæ”¯æŒéƒ¨ç½²å¤šä¸ªæ¨¡å‹
- ç‹¬ç«‹æ‰©ç¼©å®¹ï¼šæ¯ä¸ªæ¨¡å‹ç‹¬ç«‹æ‰©ç¼©å®¹
- ç»Ÿä¸€ç®¡ç†ï¼šç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ¨¡å‹

### æ¡ˆä¾‹ 2ï¼šæ¨¡å‹é‡‘ä¸é›€å‘å¸ƒ

**åœºæ™¯**ï¼šä½¿ç”¨ KServe è¿›è¡Œæ¨¡å‹é‡‘ä¸é›€å‘å¸ƒ

**å®ç°æ–¹æ¡ˆ**ï¼š

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: model-canary
spec:
  predictor:
    canaryTrafficPercent: 10
    pytorch:
      storageUri: s3://models/model-v2
    traffic: 90
    pytorch:
      storageUri: s3://models/model-v1
```

**æ•ˆæœ**ï¼š

- é‡‘ä¸é›€å‘å¸ƒï¼šé€æ­¥å‘å¸ƒæ–°æ¨¡å‹
- é£é™©æ§åˆ¶ï¼šé™ä½æ–°æ¨¡å‹å‘å¸ƒé£é™©
- å¿«é€Ÿå›æ»šï¼šå¿«é€Ÿå›æ»šåˆ°æ—§æ¨¡å‹

### æ¡ˆä¾‹ 3ï¼šè¾¹ç¼˜ AI æ¨ç†

**åœºæ™¯**ï¼šåœ¨è¾¹ç¼˜èŠ‚ç‚¹éƒ¨ç½² AI æ¨ç†æœåŠ¡

**å®ç°æ–¹æ¡ˆ**ï¼š

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: edge-ai
spec:
  predictor:
    nodeSelector:
      node-type: edge
    pytorch:
      storageUri: s3://models/edge-model
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
        limits:
          cpu: "1"
          memory: "2Gi"
```

**æ•ˆæœ**ï¼š

- è¾¹ç¼˜éƒ¨ç½²ï¼šåœ¨è¾¹ç¼˜èŠ‚ç‚¹éƒ¨ç½²æ¨ç†æœåŠ¡
- ä½å»¶è¿Ÿï¼šå‡å°‘æ¨ç†å»¶è¿Ÿ
- ç¦»çº¿æ”¯æŒï¼šæ”¯æŒç¦»çº¿æ¨ç†

---

**æ›´æ–°æ—¶é—´**ï¼š2025-11-15 **ç‰ˆæœ¬**ï¼šv1.1 **çŠ¶æ€**ï¼šâœ… åŒ…å« 2025 å¹´æœ€æ–°å®è·µ
