# Kubeflow å®‰è£…å’Œé…ç½®

## ğŸ“‘ ç›®å½•

- [Kubeflow å®‰è£…å’Œé…ç½®](#kubeflow-å®‰è£…å’Œé…ç½®)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1 æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1 æ ¸å¿ƒç»„ä»¶](#11-æ ¸å¿ƒç»„ä»¶)
  - [2 å‰ç½®è¦æ±‚](#2-å‰ç½®è¦æ±‚)
    - [2.1 Kubernetes ç‰ˆæœ¬](#21-kubernetes-ç‰ˆæœ¬)
    - [2.2 èµ„æºè¦æ±‚](#22-èµ„æºè¦æ±‚)
    - [2.3 GPU æ”¯æŒï¼ˆå¯é€‰ï¼‰](#23-gpu-æ”¯æŒå¯é€‰)
  - [3 å®‰è£…æ­¥éª¤](#3-å®‰è£…æ­¥éª¤)
    - [3.1 ä½¿ç”¨ kfctl å®‰è£…ï¼ˆæ¨èï¼‰](#31-ä½¿ç”¨-kfctl-å®‰è£…æ¨è)
    - [3.2 ä½¿ç”¨ Kubeflow Manifests å®‰è£…](#32-ä½¿ç”¨-kubeflow-manifests-å®‰è£…)
    - [3.3 ä½¿ç”¨ Helm å®‰è£…ï¼ˆç®€åŒ–ç‰ˆï¼‰](#33-ä½¿ç”¨-helm-å®‰è£…ç®€åŒ–ç‰ˆ)
  - [4 éªŒè¯å®‰è£…](#4-éªŒè¯å®‰è£…)
    - [4.1 æ£€æŸ¥ Pod çŠ¶æ€](#41-æ£€æŸ¥-pod-çŠ¶æ€)
    - [4.2 è®¿é—® Dashboard](#42-è®¿é—®-dashboard)
  - [5 é…ç½®ç¤ºä¾‹](#5-é…ç½®ç¤ºä¾‹)
    - [5.1 Pipeline ç¤ºä¾‹](#51-pipeline-ç¤ºä¾‹)
    - [5.2 Katib è¶…å‚æ•°è°ƒä¼˜ç¤ºä¾‹](#52-katib-è¶…å‚æ•°è°ƒä¼˜ç¤ºä¾‹)
  - [6 ç›¸å…³æ–‡æ¡£](#6-ç›¸å…³æ–‡æ¡£)
  - [7 2025 å¹´æœ€æ–°å®è·µ](#7-2025-å¹´æœ€æ–°å®è·µ)
    - [7.1 Kubeflow 1.9+ æ–°ç‰¹æ€§ï¼ˆ2025ï¼‰](#71-kubeflow-19-æ–°ç‰¹æ€§2025)
    - [7.2 Tekton Pipeline é›†æˆï¼ˆ2025ï¼‰](#72-tekton-pipeline-é›†æˆ2025)
    - [7.3 è¾¹ç¼˜ Kubeflow éƒ¨ç½²ï¼ˆ2025ï¼‰](#73-è¾¹ç¼˜-kubeflow-éƒ¨ç½²2025)
  - [8 å®é™…åº”ç”¨æ¡ˆä¾‹](#8-å®é™…åº”ç”¨æ¡ˆä¾‹)
    - [æ¡ˆä¾‹ 1ï¼šç«¯åˆ°ç«¯ ML Pipeline](#æ¡ˆä¾‹-1ç«¯åˆ°ç«¯-ml-pipeline)
    - [æ¡ˆä¾‹ 2ï¼šå¤šç§Ÿæˆ· ML å¹³å°](#æ¡ˆä¾‹-2å¤šç§Ÿæˆ·-ml-å¹³å°)
    - [æ¡ˆä¾‹ 3ï¼šåˆ†å¸ƒå¼è®­ç»ƒ Pipeline](#æ¡ˆä¾‹-3åˆ†å¸ƒå¼è®­ç»ƒ-pipeline)

---

## 1 æ¦‚è¿°

**Kubeflow** æ˜¯ Kubernetes åŸç”Ÿæœºå™¨å­¦ä¹ å¹³å°ï¼Œæä¾›æ¨¡å‹è®­ç»ƒã€æ¨¡å‹éƒ¨ç½²ã€å·¥ä½œæµç¼–æ’
ç­‰åŠŸèƒ½ã€‚

### 1.1 æ ¸å¿ƒç»„ä»¶

- **Kubeflow Pipelines**ï¼šæœºå™¨å­¦ä¹ å·¥ä½œæµç¼–æ’
- **Katib**ï¼šè‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜
- **KServe**ï¼šæ¨¡å‹æœåŠ¡æ¡†æ¶
- **Training Operator**ï¼šåˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- **Central Dashboard**ï¼šç»Ÿä¸€ç®¡ç†ç•Œé¢

---

## 2 å‰ç½®è¦æ±‚

### 2.1 Kubernetes ç‰ˆæœ¬

- **Kubernetes**ï¼šâ‰¥ 1.28
- **K3s**ï¼šâ‰¥ 1.30ï¼ˆè¾¹ç¼˜åœºæ™¯ï¼‰

### 2.2 èµ„æºè¦æ±‚

- **Master èŠ‚ç‚¹**ï¼šâ‰¥ 4 CPUï¼Œâ‰¥ 8 GB RAM
- **Worker èŠ‚ç‚¹**ï¼šâ‰¥ 8 CPUï¼Œâ‰¥ 16 GB RAMï¼ˆGPU èŠ‚ç‚¹éœ€è¦ GPUï¼‰

### 2.3 GPU æ”¯æŒï¼ˆå¯é€‰ï¼‰

- **NVIDIA GPU**ï¼šâ‰¥ NVIDIA T4ï¼ˆæ¨ç†ï¼‰æˆ– A100ï¼ˆè®­ç»ƒï¼‰
- **NVIDIA GPU Operator**ï¼šéœ€è¦å®‰è£… GPU Operator

---

## 3 å®‰è£…æ­¥éª¤

### 3.1 ä½¿ç”¨ kfctl å®‰è£…ï¼ˆæ¨èï¼‰

```bash
# ä¸‹è½½ kfctl
export KFCTL_VERSION=1.7.0
wget https://github.com/kubeflow/kfctl/releases/download/v${KFCTL_VERSION}/kfctl_v${KFCTL_VERSION}-linux-amd64.tar.gz
tar -xzf kfctl_v${KFCTL_VERSION}-linux-amd64.tar.gz
sudo mv kfctl /usr/local/bin/

# è®¾ç½®ç¯å¢ƒå˜é‡
export KF_NAME=kubeflow
export BASE_DIR=/opt/kubeflow
export KF_DIR=${BASE_DIR}/${KF_NAME}
export CONFIG_URI="https://raw.githubusercontent.com/kubeflow/manifests/v1.7-branch/kfdef/kfdef_k8s_istio.v1.7.0.yaml"

# åˆ›å»ºç›®å½•
mkdir -p ${KF_DIR}
cd ${KF_DIR}

# ä¸‹è½½é…ç½®æ–‡ä»¶
kfctl build -V -f ${CONFIG_URI}
kfctl apply -V -f ${CONFIG_URI}
```

### 3.2 ä½¿ç”¨ Kubeflow Manifests å®‰è£…

```bash
# å…‹éš† manifests ä»“åº“
git clone https://github.com/kubeflow/manifests.git
cd manifests

# å®‰è£… Kubeflow
while ! kustomize build example | kubectl apply -f -; do echo "Retrying to apply resources"; sleep 10; done
```

### 3.3 ä½¿ç”¨ Helm å®‰è£…ï¼ˆç®€åŒ–ç‰ˆï¼‰

```bash
# æ·»åŠ  Kubeflow Helm ä»“åº“
helm repo add kubeflow https://charts.kubeflow.org
helm repo update

# å®‰è£… Kubeflow
helm install kubeflow kubeflow/kubeflow -n kubeflow --create-namespace
```

---

## 4 éªŒè¯å®‰è£…

### 4.1 æ£€æŸ¥ Pod çŠ¶æ€

```bash
# æ£€æŸ¥æ‰€æœ‰ Pod æ˜¯å¦è¿è¡Œ
kubectl get pods -n kubeflow

# é¢„æœŸè¾“å‡ºï¼ˆéƒ¨åˆ†ï¼‰
NAME                                     READY   STATUS    RESTARTS   AGE
kubeflow-pipelines-profile-controller    1/1     Running   0          5m
katib-controller                         1/1     Running   0          5m
kserve-controller                        1/1     Running   0          5m
```

### 4.2 è®¿é—® Dashboard

```bash
# ç«¯å£è½¬å‘
kubectl port-forward -n istio-system svc/istio-ingressgateway 8080:80

# è®¿é—® Dashboard
open http://localhost:8080
```

---

## 5 é…ç½®ç¤ºä¾‹

### 5.1 Pipeline ç¤ºä¾‹

```python
from kfp import dsl

@dsl.pipeline(
    name='llm-training-pipeline',
    description='LLM training pipeline'
)
def llm_training_pipeline():
    # æ•°æ®é¢„å¤„ç†
    preprocess = dsl.ContainerOp(
        name='preprocess',
        image='preprocess:latest',
        command=['python', 'preprocess.py'],
        arguments=['--input', '/data/raw', '--output', '/data/processed']
    )

    # æ¨¡å‹è®­ç»ƒ
    train = dsl.ContainerOp(
        name='train',
        image='train:latest',
        command=['python', 'train.py'],
        arguments=['--data', '/data/processed', '--output', '/models/llm'],
        resources={
            'gpu': 1,
            'memory': '32Gi'
        }
    )
    train.after(preprocess)

    # æ¨¡å‹éªŒè¯
    validate = dsl.ContainerOp(
        name='validate',
        image='validate:latest',
        command=['python', 'validate.py'],
        arguments=['--model', '/models/llm', '--data', '/data/test']
    )
    validate.after(train)
```

### 5.2 Katib è¶…å‚æ•°è°ƒä¼˜ç¤ºä¾‹

```yaml
apiVersion: kubeflow.org/v1beta1
kind: Experiment
metadata:
  name: llm-hyperparameter-tuning
spec:
  algorithm:
    algorithmName: bayesian-optimization
  parameters:
    - name: learning-rate
      parameterType: double
      feasibleSpace:
        min: "0.001"
        max: "0.1"
    - name: batch-size
      parameterType: int
      feasibleSpace:
        min: "16"
        max: "128"
  objective:
    type: maximize
    objectiveMetricName: accuracy
  parallelTrialCount: 3
  maxTrialCount: 20
  trialTemplate:
    trialSpec:
      apiVersion: batch/v1
      kind: Job
      spec:
        template:
          spec:
            containers:
              - name: training-container
                image: train:latest
                command:
                  - python
                  - train.py
                  - --lr=${trialParameters.learning-rate}
                  - --batch-size=${trialParameters.batch-size}
```

---

## 6 ç›¸å…³æ–‡æ¡£

- [`README.md`](README.md) - AI/ML å®ç°ç»†èŠ‚æ€»è§ˆ
- [`../../02-views/10-quick-views/ai-ml-architecture-view.md`](../../02-views/10-quick-views/ai-ml-architecture-view.md) -
  AI/ML æ¶æ„è§†è§’
- [`gpu-scheduling.md`](gpu-scheduling.md) - GPU èµ„æºè°ƒåº¦é…ç½®

## 7 2025 å¹´æœ€æ–°å®è·µ

### 7.1 Kubeflow 1.9+ æ–°ç‰¹æ€§ï¼ˆ2025ï¼‰

**æœ€æ–°ç‰ˆæœ¬**ï¼šKubeflow 1.9+ï¼ˆ2025 å¹´ï¼‰

**æ–°ç‰¹æ€§**ï¼š

- **ç»Ÿä¸€ Pipeline å¼•æ“**ï¼šç»Ÿä¸€ä½¿ç”¨ Tekton Pipeline
- **å¤šç§Ÿæˆ·å¢å¼º**ï¼šæ›´å¥½çš„å¤šç§Ÿæˆ·æ”¯æŒ
- **æ€§èƒ½ä¼˜åŒ–**ï¼šPipeline æ‰§è¡Œæ€§èƒ½æå‡ 40%

**å®‰è£…æœ€æ–°ç‰ˆæœ¬**ï¼š

```bash
# å®‰è£… Kubeflow 1.9
kubectl apply -k "github.com/kubeflow/manifests/example?ref=v1.9.0"
```

### 7.2 Tekton Pipeline é›†æˆï¼ˆ2025ï¼‰

**2025 å¹´è¶‹åŠ¿**ï¼šKubeflow å…¨é¢é‡‡ç”¨ Tekton Pipeline

**ä¼˜åŠ¿**ï¼š

- **äº‘åŸç”Ÿ**ï¼šå®Œå…¨åŸºäº Kubernetes
- **å¯æ‰©å±•**ï¼šä¸°å¯Œçš„æ‰©å±•æœºåˆ¶
- **æ ‡å‡†åŒ–**ï¼šéµå¾ª CDF æ ‡å‡†

**é…ç½®ç¤ºä¾‹**ï¼š

```yaml
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: ml-pipeline
spec:
  tasks:
  - name: train
    taskRef:
      name: train-task
  - name: evaluate
    taskRef:
      name: evaluate-task
    runAfter:
      - train
```

### 7.3 è¾¹ç¼˜ Kubeflow éƒ¨ç½²ï¼ˆ2025ï¼‰

**2025 å¹´è¶‹åŠ¿**ï¼šåœ¨è¾¹ç¼˜èŠ‚ç‚¹éƒ¨ç½² Kubeflow

**é…ç½®ç¤ºä¾‹**ï¼š

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeflow-edge
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kubeflow-edge
  template:
    spec:
      nodeSelector:
        node-type: edge
      containers:
      - name: kubeflow
        image: kubeflow/kubeflow:1.9.0
```

## 8 å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šç«¯åˆ°ç«¯ ML Pipeline

**åœºæ™¯**ï¼šæ„å»ºç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹  Pipeline

**å®ç°æ–¹æ¡ˆ**ï¼š

```yaml
apiVersion: kubeflow.org/v1
kind: Pipeline
metadata:
  name: end-to-end-ml
spec:
  description: "ç«¯åˆ°ç«¯ ML Pipeline"
  tasks:
  - name: data-preprocessing
    taskRef:
      name: preprocess-task
  - name: model-training
    taskRef:
      name: train-task
    dependencies:
      - data-preprocessing
  - name: model-evaluation
    taskRef:
      name: evaluate-task
    dependencies:
      - model-training
  - name: model-deployment
    taskRef:
      name: deploy-task
    dependencies:
      - model-evaluation
```

**æ•ˆæœ**ï¼š

- è‡ªåŠ¨åŒ–ï¼šç«¯åˆ°ç«¯è‡ªåŠ¨åŒ– Pipeline
- å¯é‡ç°ï¼šPipeline å¯é‡ç°
- å¯æ‰©å±•ï¼šæ˜“äºæ‰©å±•æ–°ä»»åŠ¡

### æ¡ˆä¾‹ 2ï¼šå¤šç§Ÿæˆ· ML å¹³å°

**åœºæ™¯**ï¼šåœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­éƒ¨ç½² Kubeflow

**å®ç°æ–¹æ¡ˆ**ï¼š

```yaml
apiVersion: kubeflow.org/v1
kind: Profile
metadata:
  name: tenant-a
spec:
  owner:
    kind: User
    name: tenant-a@example.com
  resourceQuotaSpec:
    hard:
      requests.cpu: "10"
      requests.memory: "20Gi"
      limits.cpu: "20"
      limits.memory: "40Gi"
```

**æ•ˆæœ**ï¼š

- ç§Ÿæˆ·éš”ç¦»ï¼šæ¯ä¸ªç§Ÿæˆ·æœ‰ç‹¬ç«‹çš„å‘½åç©ºé—´
- èµ„æºæ§åˆ¶ï¼šé€šè¿‡ ResourceQuota æ§åˆ¶èµ„æº
- ç»Ÿä¸€ç®¡ç†ï¼šç»Ÿä¸€ç®¡ç†æ‰€æœ‰ç§Ÿæˆ·

### æ¡ˆä¾‹ 3ï¼šåˆ†å¸ƒå¼è®­ç»ƒ Pipeline

**åœºæ™¯**ï¼šä½¿ç”¨ Kubeflow è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ

**å®ç°æ–¹æ¡ˆ**ï¼š

```yaml
apiVersion: kubeflow.org/v1
kind: MPIJob
metadata:
  name: distributed-training
spec:
  slotsPerWorker: 1
  runPolicy:
    cleanPodPolicy: Running
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          containers:
          - image: train:latest
            name: launcher
            command:
            - mpirun
            - python
            - train.py
    Worker:
      replicas: 4
      template:
        spec:
          containers:
          - image: train:latest
            name: worker
            resources:
              limits:
                nvidia.com/gpu: 1
```

**æ•ˆæœ**ï¼š

- åˆ†å¸ƒå¼è®­ç»ƒï¼šæ”¯æŒå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒ
- GPU åŠ é€Ÿï¼šæ”¯æŒ GPU åŠ é€Ÿè®­ç»ƒ
- è‡ªåŠ¨æ‰©ç¼©å®¹ï¼šæ ¹æ®è´Ÿè½½è‡ªåŠ¨æ‰©ç¼©å®¹

---

**æ›´æ–°æ—¶é—´**ï¼š2025-11-15 **ç‰ˆæœ¬**ï¼šv1.1 **çŠ¶æ€**ï¼šâœ… åŒ…å« 2025 å¹´æœ€æ–°å®è·µ
